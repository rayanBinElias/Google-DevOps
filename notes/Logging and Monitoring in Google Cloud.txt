Module 1:Introduction to Google CLoud's Operations Suite
  - In this module, we will take some time to do a high-level overview of the various products which comprise Google Cloud’s logging, 
  monitoring, and observability suite.

  Learning Objectives
    1. Describe the purpose and capabilities of Google Cloud’s operations suite
    2. Explain the purpose of the Cloud Monitoring tool.
    3. Explain the purpose of Cloud Logging and Error Reporting tools.
    4. Explain the purpose of Application Performance Management tools.
    5. Explain the purpose of Cloud Trace

  Need for google Cloud observability
    1. Visibility into system health
      - help me understand my app and tell if it's healthy

    2. Error reporting and alerting
      - bring problems directly to my attention

    3. Efficient troubleshooting
      - help me fix it if it's broken

    4. Performance improvement

    Monitoring
      - gives you real-time sys information
      - collecting, processing, aggregating and displaying real-time quanitative data about a sys such as:
        - query counts and types
        - error counts and types
        - processing times
        - server lifetimes
      - foundation of product reliability

    What's needed from products?
      1. Continual improvement
      2. Dashboards
      3. Automated alerts
      4. Incident response

    Four golden signals
      1. latency
        - changes in latency could indicate emerging issues
        - its values may be tied to capacity demands
        - it can be used to measure sys improvements

        How?
          - page load latency
          - number of req waiting for a thread
          - service response time
          - transaction duration
          - time to first response 
          - and time to complete data return.

      2. Traffic
        - indicator of current sys demands
        - historical trends are used for capacity planning
        - core measure when calculating infra spend

        How?
          -  number of HTTP requests per second
          - number of requests for static vs. dynamic content
          - number of concurrent sessions

      3. Saturation
        - indicator of how full the service is
        - focuses on the most constrained resources
        - freqntly tied to degrading performance as capacity is reached
        
        How?
          - percentage memory utilization
          - percentage of thread pool utilization
          - percentage of cache utilization

      4. errors
        - indicate configuration or capacity issues
        - indicate service level objective violations
        - error might mean it's time to send out an alert

        How?
         - number of 400/500 HTTP codes
         - number of failed requests
         - number of exceptions 

    Observability steps
      1. Capture signals
        - metrics
        - Logs
        - Trace
      
      1.1 Manage Incidents
        - alerts
        - error reporting
        - SLO
      
      2. Visualze and Analyze
        - Dashboards
        - metrics Explorer
        - logs Explorer
        - service monitoring
        - log analytics
        - health checks

      3. Troubleshoot
    
  Cloud Monitoring
    - rovides visibility into the performance, uptime, and overall health of cloud-powered applications. 
    - It collects metrics, events, and metadata from projects, logs, services, systems, agents, custom code, and various common application components, including 
      - Cassandra
      - Nginx
      - Apache Web Server
      - Elasticsearch
    - Monitoring ingests that data and generates insights via dashboards, Metrics Explorer charts, and automated alerts

    Feautures
      - Many free metrics
      - Open source standards
      - customization for key workloads
      - in-context visualizations & alerts

  Cloud  Logging
    - allows users to collect, store, search, analyze, monitor, and alert on log entries and events. 
    - It provides automatic ingestion with simple controls for routing, storing, and displaying your log data. 
    - It leverages tools like Log Analytics to view trends, or Error Reporting and Log Explorer to quickly examine problems

    Multiple aspects
      - collect
        - cloud events
        - config changes
        - customer services
        - logs at various level of the resource hierarchy
     
     - analyze
        - log data in real time w/ integrated logs Explorer
        - run queries and analyze w/ log analytics
        - exported logs from 
          - GCS
          - bigquery
      
      - export
        - export to 
          - GCS
          - bigquery
          - pub/sub messages
            - can be anlayzed in near-real time using custom code or stream processing like dataflow
        - log-based metrics fro augemented monitoring

      - retain
        - data access and service logs for 30 days (configurble upto 3650 days)
        - admin logs for 400 days by default
        - longer term in GCS/bq

    Use case
      1. Developers
        - troubleshooting
        - debugging
       
        - Integration into popular SDKs
          - get started quickly w/ a large collection of sys metrics and logs
        
        - real time log analysis
          - analyze log data in real time, devug code, troubleshoot your apps
        
        - quick error detection
          - find errors via stack traces automatically w/ error reporting

      2. Operator
        - SLO/alerting
        - log Management
        - workload Management
        - cost Management
        
        - Collect the right telemetry
          - instrumentation for GCE, on-prem and other cloud providers

        - Centralize logs
          - centralize logs for specific users, teams and/or org

        - Manage logs
          - set retention periods, select supported regions for regional data storage

        - Set alerts
          - understand log volume/cost, set alerts on important app metrics

        - Export logs
          - export to Google CLoud for storage, analysis and integration w/ 3rd parties

      3. SecOps Analyst
        - Secops of google cloud fleet of resources
        - uses platform features to meet org sec requirements

        - collect auidt logs
          - collect google cloud audit logs by default, advanced sec logs such as data access logs

        - collect network telemetry data
          - collect and analyze VPC flow logs, GKE network, firewall, load balancer logs

        - analyze logs for security events
          - view audit logs and other events to investigate possible sec events

  Error Reporting
    - identifies, counts, analyzes and aggregates the crashes in your running cloud services

    features
      - real-time processing
        - app arrors are processed and displayed w/in seconds

      - quickly view and understand errors
        - dedicated page displays the details of the error

      - instant notification
        - notified when events occur

      - dedicated view
        - time charts
        - occurences
        - affected user counts
        - first and last seen dates
        - cleaned exception stack trace

      - alerting policy

  Application Performance Management Tools
    1. Cloud Trace
      - collects latency data from distributed app and displays it in the GC console
      - captures traces from app deployed
        - App engine flexible and standard env
        - GCE VMs
        - GKE containers
        - Cloud run
        - non GC env

      Latency Reports
        - provide performance insights in near-real time
        - generate in-depth latency reports to surface performance degradations
        - identify recent changes to app performance

    2. Cloud Profiler
      - uses statistical tech and extremely low-impact instrumentation to provide complete picture of app
      - allows dev to analyze app running anywhere
      - presents call hierarchy and resource consumption of relevant function in an interactive flame graph

  Google Cloud's Operation suite
    - helps you explore the known and unknown issues
    
    - user-focused products
      - SLO monitoring, uptime checks, tracing

    - open,flexible foundations
      - prometheus
      - opentelemetry
      - fluentbit

    - integrated for ease
      - auto 
        -ingest log
        - connect sets
        - collect in-context telemetry across GC services

    - meaningful analysis and alerting
      - use powerful analysis tools
      - leverage alerting for both automated and human-led resolutions

  Quiz
    1. You want a simple way to see the latency of requests for a web application you deployed to Cloud Run. What Google Cloud tool should you use?
      - Trace

    2. You want to examine messages generated by running code. Which tool might be best for doing this?
      - Logs Explorer

    3. Users have reported that an application occasionally returns garbage data instead of the intended results, but you have been unable to reproduce this problem in your test environment. Which tool might be of best help?
      - Error Reporting
      
      - Logs Explorer

    4. You want to calculate the uptime of a service and receive alerts if the uptime value falls below a certain threshold. Which tool will help you with this requirement?
      - Cloud Monitoring

      - Profiler

Module 2: Monitoring Critical systems
  - Monitoring is all about keeping track of exactly what's happening with the resources we've spun up inside of Google's Cloud. 
  In this module, we'll take a look at options and best practices as they relate to monitoring project architectures. We'll...

  Learning Objectives
    1. Use Cloud Monitoring to view metrics for multiple cloud projects.
    2. Explain the different types of dashboards and charts that can be built.
    3. Create an uptime check.
    4. Explain the cloud operations architecture.
    5. Explain and demonstrate the purpose of using Monitoring Query Language (MQL) for monitoring.

  Cloud Monitoring architecture patterns
    Three layers
      1. Data collection layers
        - collects  
          - metrics
          - logs
          - traces
      
      2. Data storage layer
        - stores the collected data
        - routes to the configured visualization and analysis layer
        - includes the cloud monitoring API
          - helps triage the metrics collected to be stored for future analysis
      
      3. data analysis and visualization layer
        - analyzes the collected data to identify
          - problems
          - trends 
          - presents the analyzed data in a way that is easy to understand
          - dashboards
            - visualize data
          - uptime checks
            - monitor app
          - alerting policies
            - config alerts
            - notify events or email

        Notes
          Platform monitoring/Cloud Monitoring
            - no copst for all system metrics

          Application monitoring - GKE
            - integrates w/:
              - cloud logging
              - cloud monitoring
              - google cloud managed service for prometheus
                - collects data

              Google Managed Prometheus(GMP)
                - part of cloud monitoring as aprometheus data
                - promql compatible query language

          Application monitoring - GCE
            - ops agents to collect data
            - partner with 
              - Datadog/NewRelic
              - BindPlane by Blue Medora for logs collection
  
  Monitoring multiple projects
    - only one single project in metric scope by default

    Scoping project
      - hosts a metrics scope
      - stores
        - alerts
        - uptime checks
        - dashboards
        - monitoring groups

    Notes
      - Production deployment
        - create a separate projects for monitoring

        Why?
          - single pane of glass that provides visibility into the enire group of related projects
            - monitoring config for other project is retain if fe/be/db are deleted

      monitoring.viewer
        - access to dashboards and access to all data by default
          - including all other projects that are monitored by that metrics scope

  Data model and dashboards
    Data model
      time-series data
        - metric
          - metric label
          - metric type
          - desicribes the metric
        - resource
          - resopurce-label
          - resource info
        - metricKind and valueType
          - how to interpret the values
        - points
          - values of the metrics

  Query metrics 
    - using MQL and promql

    MQL
      - query cloud monitoring time-series data by using text-based interface
      - manipulate, retrieve and perform complex operations on time-series data
    
    PromQL
      - query metrics from Google Cloud Managed Service for Prometheus
      - Query system metrics from GKE and Compute engine
      -Integra

  Uptime checks
    - test availability of your public services from locations around the world
    - can help us ensure that external facing services are running 
    and we aren't burning our error budgets
    - can be set to
      - HTTP
      - https
      - TCP
    - can check 
      - app engine app
      - GCE instance
      - URL of a host
      - AWS instance
      - load balancer
    - can create alerting policy and budget alert

Budgets & alerts
  - Select a billing account to go to the budgets & alerts page, 
  where you can create budgets to monitor all of your Google Cloud charges in one place.
    
  Qwiklabs: Monitoring and Dashboarding Multiple Projects from a Single Workspace
    - Google Cloud Monitoring empowers users with the ability to monitor multiple projects from a single metrics scope.
    In this exercise, you start with three Google Cloud projects, two with monitorable resources, and the third you use to host a metrics scope.
    You attach the two resource projects to the metrics scope, build uptime checks, and construct a centralized dashboard.

    - Monitoring and Dashboarding Multiple Projects
    - https://www.coursera.org/learn/logging-monitoring-observability-google-cloud/gradedLti/yxggp/lab-monitoring-and-dashboarding-multiple-projects-from-a-single-workspace

    Objectives
      - Configure a Worker project.
      - Create a metrics scope and link the two worker projects into it.
      - Create and configure Monitoring Groups.
      - Create and test an uptime check.

    Task 1: Configure the resource projects
      Install Nginx
        sudo apt-get update

        sudo apt-get install -y nginx

        ps auwx | grep nginx
        

    Task 2: Create a metrics scope and link the two worker projects into it
      ADD GCP PROJECTS.

Click Select Project and select the Worker 1 and Worker 2 projects.

    Task 3: Create and configure Monitoring groups

    Task 4: Create and test an uptime check

    Task 5: Create a custom dashboard
      sudo apt-get update
      sudo apt-get install apache2-utils

      curl 100,000
        ab -s 120 -n 100000 -c 100 $URL/

      curl 500,000
        ab -s 120 -n 500000 -c 500 $URL/
    
  Summary
    1. use cloud monitoring to view metrics for multiple cloud projects

    2. explain the different types of dashboards and charts that can be built

    3. create an uptime check

    4. explain the purpose of using MQL for monitoring

  Quiz
    1. You want to analyze the error rate in your load balancing environment. Which interface helps you view a chart with a ratio of 500 responses to all responses
      - MQL

      - Metrics Explorer

    2. You want to be notified if your application is down. What Google Cloud tool makes this easy?
      - Uptime check

    3. What is the name of the project that hosts a metrics scope?
      - Scoping project

      - Hosting project

Module 3: Alerting policies
  - Alerting gives timely awareness to problems in your cloud applications so you can resolve the problems quickly. 
  In this module, you will learn how to develop alerting strategies, define alerting policies, add notification channels, identify types of...

  Learning Objectives
    - Explain alerting strategies.
    - Explain alerting policies.
    - Explain error budget.
    - Explain why server-level indicators (SLIs), service-level objectives (SLOs), and service-level agreements (SLAs) are important.
    - Identify types of alerts and common uses for each.
    - Use Cloud Monitoring to manage services.

  SLI, SLO and SLAs
    SLIs
      - must be a number or a delta, something we can measure and place in a mathematical equation
        - Error rate

    SLOs
      - 99.5 or 99.9%

      Criteria
        - S: specific
        - M: measurable
        - A: achievable
        - R: relevant
        - T: time-bound

    SLAs
      - commitments made to your customers
        - that sys and app will have only a certain amt of downtime

    Deciding SLA, SLO and SLI
      Indicators
        http get / latency

      Objectives
        200ms

      Agreement 
        300ms

  Developing an alerting strategy

  Creating alerts

  Qwiklab: Alerting in Google Cloud
    - In this lab, you deploy an application to App Engine 
    and then create alerting policies to notify you 
    if the application is not accessible or is generating errors.

    - https://googlecoursera.qwiklabs.com/focuses/35934121?parent=lti_session

    Objectives
      In this lab, you learn how to perform the following tasks:

      - Download a sample app from GitHub.
      - Deploy an application to App Engine.
      - Create uptime checks and alerts.
      - Optionally, create an alerting policy with the CLI.

    Task 1 Download and test a sample app from GitHub
      //create app engine region
      gcloud app create --region=us-east4

      // deploy in app engine
      gcloud app deploy --version=one --quiet
   
    Task 2: Deplpy an app to app engine
    
    Task 3. Examine the App Engine logs  

    Task 4. Create an App Engine latency alert
      //redeploy new version GAE
      gcloud app deploy --version=two --quiet

      //generate some consistent load, in Cloud Shell, enter the following command:
      while true; do curl -s https://$DEVSHELL_PROJECT_ID.appspot.com/ | grep -e "<title>" -e "error";sleep .$[( $RANDOM % 10 )]s;done

      - This command makes requests to the App Engine app continuously in a loop. The grep command will display the title of the page when the request works. It also displays the error, if it doesn’t work. Every iteration, the thread sleeps a random amount of time less than a second, but with the 10s response time delay it will seem much longer.

    Task 5. (Optional) Creating an Alerting Policy with the CLI
      //app-engine-error-percent-policy.json.
        https://googlecoursera.qwiklabs.com/focuses/35934121?parent=lti_session

      //Deploy alerting policy w/ gcloud
        gcloud alpha monitoring policies create --policy-from-file="app-engine-error-percent-policy.json"
 
  Service Monitoring
    - a sing pane summary of the health of your various services.
    
    Service Monitoring answers the following:
      What are the commitments regarding the availability of those services?

      Are your service meeting them?

      For microservices-based apps,what are the inter-service dependencies?

      How to double check new code rollouts and triage problems if a service degradation occurences

      Can you look at all the monitoring signals for a service holistically to reduce the MTTR(meant time to repair)?

    SLO compliance Calculation approaches
      1. Windows-based
          - each window represents a data point, instead of all the data points that comprise the windo
      2. Request-based

    Windows-based Vs Request-based SLOs
      Windows-based
        = total number of good reuqest : total number of bad requests

        Example
          1. 95 percentile SLO = latency lesss than 100ms for 99% of 10 min Windows
          2. 99.9% request-based SLO can allow 1,000 bad requests every 30 days
          3. 99.9% windows-based SLO based on a 1-minute window can allow a total of 43 bad windows.
            - 43,200 total windows * 99.9% = 43,157 good windows
          4. Windows-based SLOs can be tricky, because they can hide burst-related failures 
      Request-based
        = good request : total requests

        Example
          request based SLO = latency below 100ms for 95% of requests

    availability
      - ratio of number of successful responses to the number of all responses

    Latency
      - ratio of number of callss that are below the specified latency threshold to the number of all calls

  Qwiklab: Service Monitoring
    - Google Cloud's Service Monitoring streamlines the creation of microservice Service Level Objectives (SLOs) 
    based on availability, latency, or custom Service Level Indicators (SLIs). 
    In this lab, you use Service Monitoring to create a 99.5% availability SLO 
    and corresponding alert.

    Objectives
      - In this lab, you learn how to perform the following tasks:

      - Deploy a test application.
      - Use Service Monitoring to create an SLO.
      - Tie an alert to the SLO.

    Task 1. Deploy a test application

    Task 2. Use Service Monitoring to create an availability SLO
      In this task, you:
        - Use Service Monitoring to create an availability SLO.
        - Create an alert tied to your SLO.
        - Trigger the alert.


  Quiz
    1. In evaluating your alerting policies, which below best describes precision? 
      - The proportion of events detected that were significant.

    2. Explain error budget.
      - The proportion of alerts detected that were relevant to the sum of relevant alerts and missed alerts.

      - How long alerts fire after an issue is resolved.

    3. In the statement “Maintain an error rate of less than 0.3% for the billing system”, what is an SLI?
      - Error rate
      
      - 0.3%

      - Less than 0.3%

Module 4: Advanced logging and analysis
  - In this module, we will examine some of Google Cloud's advanced logging and analysis capabilities.
  Specifically, in this module you will learn to identify and choose among resource tagging approaches, define log sinks, create monitoring metrics...

  Learning Objectives
    - Use Log Explorer features.
    - Explain the features and benefits of logs-based metrics.
    - Define log sinks (inclusion filters) and exclusion filters.
    - Explain how BigQuery can be used to analyze logs.
    - Export logs to BigQuery for analysis.
    - Use log analytics on Google Cloud.

  Cloud Logging overview and architecture
    - allows you to sore, search, analyze, monitor and alert on log data and envents from Google CLoud
    - a fully managed service that performs at scale and can ingest app and sys log data from thousands of VMs

    Logs
      - pulse of your workloads and app

    - helps you with the following:
      1. Gather data from various workloads
        - gathers the info req to troubleshoot and understand the workload and app needs
      2. Analyze large volumes of data
        - tools like:
          - error reporting
          - log Explorer
          - log analytics
        - let you drive insights from large sets of data
      3. Route and store logs
        - route your logs to the region or service of your choice for add compliance or business benefits
      4. Get compliance insights
        - leverage audit and app logs for compliance patterns
      
      Cloud logging architecture
        1. Log collection
           - log data originates
            - GCE
              - ops agent
            - App engine
            - GKE logs
              - fluentbit-based collector
            - GC services
        2. Log Route
          - determines which log data is routed to each destination

          Logging API

          Cloud Logging Router

          Log sinks
            - destinations where log data is stored

          Pub/sub topics
            - used to route log data to other sercices such as bq

          bq
            - fully managed petabyte scale analytics data warehouse that can be used
            to store and analyze log data

        3. Log store
            - GCS buckets
              - provides storage of log data in GCS

            - cloud logging log buckets
              - sotrage buckets designed for storing log data
            
            Log entries
              - stored as JSON filters

        4. Log Visualize and Analyze
          Log Explorer
            - optimized for troubleshooting uses cases w/ features like:
              - log streaming
              - log resource Explorer
              - histogram for visualization
          
          Error Reporting
            - help users react to critical app errors through automated error grouping and notifications

          Log-based metrics
            - dashboards and alerting provide other ways to understand and make logs actionable

          Log Analytics
            - Expands the toolset to include ad hoc log analysis capabilities

          Dashboards

          Notifications


  Log types and collection
    Log types:
      1. platform logs
        - logs written by GC services
        - help you debug and troubleshoot issues
        - help you better understand GC services
        
        VPC flow logs
          - record sample of network flows sent from and received by VM instances
      
      2. component logs
        - same as platform but generated by google-privided softtware components runniung on your sys

      3. security logs
        - can answer "who did what, where and when"

        Cloud Audit logs
          - provide infor about admin act and acesses w/in google cloud services

        Acct transparency
          - provides you w/ logs of actions taken by google staff when accessing your google cloud content
          
      4. user-written logs
        - logs written by custom app and services
          - Ops Agent
          - Cloud Logging API
          - CLoud logging client libraries
        
        Cloud run/functionalities
          - simple runtime logging by diffault
      
      5. multi/hybrid cloud logs
        GKE
          - agents auto collect logs from stdout and stderr

          logs written to stdout or stderr
            - appear automatically in the google cloud console

        GCE
          - agents collect logs from known locations or logging services like 
            - windows event log
            - journald
            - syslogd

  Storing, routing and exporting the logs
    Process steps:
      1. Centralized logging apis
        - cloud logging receives log entries through here

      2. Log sinks
        - det log destination with config like retention

        types
          - _required
            - routes admin activity, sys event and access transparency logs automatically
            - no charges
            - retention period: 400 days non configuratable
            - can't be deleted or modified

          - _default
            - logs not ingested by _required bucket are routed here
            - no cost charges
            - configurable retention period
            - cannot be deleted but can be disabled
          
          - _user-defined

      3. Storage
          - GCS
            - diff thatn cloud logging buckets
          - bq
          - pub/sub
            - to third party app

    Log storage page
      - summary of statistics fo the ffg:
        - current toal volume
          - amt of logs your project has received since the first date of the current month
        - prev month volume
          - amt of logs your project received in the last calendar month
        - projected volume by EOM
          - estimated amt of logs your project will receive by the end of the current month based on current usage
        - metrics Explorer
          - build charts for any metric collected by your project

      Cloud logging bucket
        - works well to help pre-separate log entries into a distinct log storage bucket

      BQ dataset
        - allows the SQL query power of BQ to be brought to bear on large and complex log entries

      Cloud Sotage bucket
        - for long term storage processing w/ other sys

      Pub/sub topics
        - can export log entries to msg handling third party app/sys like  dataflow/cloud function

      Splunk
        - used to integrate logs into existing splunk based sys
        - is a tool for collecting, monitoring, visualizing and analyzing machine data from any source. You may receive faster responses at answers.splunk.com which is actively monitored by Splunk employees

      Other project
        - useful to help control access to a subset of log entries

      Log sink creation lifecycle
        - sink destination
        - logs filter
  
    Log Explorer
      - use to build aquery that selects the logs you want to exclude unwatend entries out of the sink

    Example Pipeline
      Real-time Log archiving and analysis
        1. events
        2. logging
        3. pub/sub
        4. dataflow
          - real time log processing at scale
          - react to realtime issues while streaming it to bigquery for longer-term analysis
        5. bigquery

      archive logs for long-term storage
        1. events
        2. logging
        3. cloud storage

      Exporting back to splunk
        1. events
        2. logging
        3. pub/sub
        4. splunk

    Notes
      - Centralized Log aggregation for:
        - auditing
        - retention
        - non repudiation

      - Aggregation levels
        - Project
          - exports all logs for specified project
          - log filter for choosing log types
        - folder
          - inclludes logs from folders, subfolders/projects
        - organization
          - inclludes logs from folders, subfolders/projects

    Security log analytics workflow recommends aggregated sinks
      1. Collect
        - cloud logging
      2. Route
        - aggregated sink
          - pub/sub
          - cloud storage
      3. Analyze
        - log analytics
        - bigquery
        - third party SIEM
        - Chronicle SIEM

  Query and view log
    Log Explorer
      - lets you retrieve logs, parse and analyze log data
      - refine your query parameters

      1. Action toolbar
        - refine logs to projects or storage views
        - share a link and learn about logs explorer
        
      2. Query pane 
        - you can build queries
        - view recently viewed and saved queries 

      3. Results toolbar
        - used to quickly show or hide logs and histogram pane
        - create a log based metric or alert
        - now options
          - helps query and view the current time results

      4. query results
        - used to quickly show or hide logs and histogram pane and create a log based metric or alert

      5. log fields
        - used to filter your options based on various factors such as resource type, log name, project ID, etc. 
        - refine query
        - shows the count of log entries

      6. histogram
        - where the query result is visualized as histogram bars
        - each bar is a time range and is color coded based on severity
        - lets you visualize the distribution of logs over time
          - makes it easier to see trends in your logs data and torubleshoot problems

      Log query language(LQL)
        - used to create queries
  
      Log comparison operators 
        =
          equals
        !=
          does not equal
        ><>=<=
          numeric ordering
        :
          has
        :*
          presence
          
        =~
          search for a pattern
        !~
          search not for pattern

      The recipe for finding entries
        1. What do you know about the log entry?
          - log filename
          - resource
          - some text
        2. full text searches are slow but may be effective:
          - /score called
        3. use indexed SEARCH function fro complete text matches, because they perform a case-insensitive match
          - SEARCH(textPayload, "hellow world")
        4. if possible, restrict text searches to a log field
          - jsonPayload:"/score called"
          - jsonPayload.message="/score called"

      Finding entries quickly
        1. Search on an indexed field
          - httpRequest.status 
          - logName
          - operation.id
          - resource.type
          - timestamp
          - severity
        
        2. Apply constrains on resource.type and resource.labels field
          - resource.type = "gke_cluster"
          - resource.labels.namespace = "my-cool-namespace"

        3. Be specific on which logs you're searching
          - logName = "projects/benkelly-test/logs/apache-access"

        4. limit the time range that you're searching
          - timestamp >= "2018-08-08T10:00:00Z" AND timestmap <= "2018-08-08T10:10:00Z"

  Using log-based metrics
    - derive metric data from the content of log entries

    Life cycle
      1. log entries
      2. cloud logging
      3. filter
      4.1 count
      4.2 extract
      5. log based metric
      6. cloud monitoring

    Use cases:
      1. Count the occurrences
        - count the occurrences of a message like:
          - warning or error
          - logs
          - receive a notification when number of occurences crosses a threshold

      2. Observe trends in your data
        - observe trends in your data like:
          - latency values in your logs
          - receive notification if the values change in an unacceptable way

      3. Visualize extracted data
        - create charts to display the numeric data extracted from your logs

    Key access control roles
      1. logs configuration writer
        - list
        - create
        - get
        - update
        - delete log-based metrics
      
      2. logs viewer
        - view existing logs

      3. monitoing viewer
        - read the time series in log based metrics

      4. logging admin, editor and owner
        - broad-level roels that can create log-based metrics

    Log based metrics types
      1. counter metrics
        - count the number of matched log entries

      2. distribution metrics
        - record the statistical distinution of the extracted log values

      3. boolean metrics
        - record where a log entry matches a specified filter

    Scope of log-based metrics
      - sys-defined log-based metrics apply at the project level
      - user-defined log based metrics apply either at project or bucket level

    basic lifecycle flow of log based metrics
      1. find the log w/ requisite data
      2. filter tot he required entries
      3. actions | create metric
      4. pick a metric type(counter or distribution)
      5. if distribution, set configuration
      6. you can also add labels
        - labels cannot be deleted once created
        - upto 10 user defined labels - ametric cannot be removed
        - 30,000 active time series
        - each level upto 2000 per metrics
    
    Note  
      field name
        - enter the name of the log entry field that contains value of the label

  Log analytics
    - gives you the analytical power of BQ w/in cloud logging console

    Use cases
      1. Troubleshooting
        - get to the root cause w/ search filtering, histogram and suggested searches

      2. Log analysis
        - analyze app performance, data access and anetwork access patterns

      3. Reporting
        - use the logs data in Log analytics directly from BQ to report on aggregated app

    analytics-enabled bucket log data VS log routed to bigquery
      - log data in bq is managed by cloud logging
      - bq ingestion and storage costs are included in your logging costs.
      - data residency and lifecycle are managed by cloud logging

    Notes:
      - you can't downgrade the log bucket to remove the use of log analytics

    Usecases by role
      1. DevOps 
        - reduce MTTR by using advanced analtyical capabilities to diagnose issues
        - help me quickly troubleshoot an issue by looking at the top count of requests grouped by response type and severity

      2. Security
        - investigate security-related attacks w/ queries over large volumes of security logs
        - help me find all the audit logs associated w/ specific user over the past month
         
      3. IT or Network Operations
        - provide better network insight and management through advanced log aggregation capabilities
        - help me identify network issues for GKE instances using VPC and firewall rules


  Qwiklab: Log analytics on Google Cloud
    - In this lab you will learn about the features and tools provided by Cloud Logging to gain insight of your applications.
    - https://googlecoursera.qwiklabs.com/focuses/35957393?parent=lti_session

    What you'll learn
      - How to use Cloud Logging effectively and get insight about applications running on GKE
      - How to effectively build and run queries using log analytics

    Microservices demonstrate
      https://github.com/GoogleCloudPlatform/microservices-demo

    Task 1: Infra setup
      // set default region
        gcloud config set compute/zone us-east4-c
      
      //see cluster's status:
        gcloud container clusters list

      //get cluster credentials
        gcloud container clusters get-credentials day2-ops --region us-east4

      //verify if nodes have been created
        kubectl get nodes

    Task 2: Deploy App
      git clone https://github.com/GoogleCloudPlatform/microservices-demo.git

      cd microservices-demo

      //install app
        kubectl apply -f release/kubernetes-manifests.yaml

      //confirm pods is running
        kubectl get pods

      /// get external ip of app
        export EXTERNAL_IP=$(kubectl get service frontend-external -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
echo $EXTERNAL_IP

      // configrm app is running
        curl -o /dev/null -s -w "%{http_code}\n"  http://${EXTERNAL_IP}

    Task 3: Manage log buckets

    Task 4: Log analysis

      // find the most recent errors in log analytics
        SELECT
          TIMESTAMP,
          JSON_VALUE(resource.labels.container_name) AS container,
          json_payload
        FROM
          `qwiklabs-gcp-01-cb39db4e6168.global.day2ops-log._AllLogs`
        WHERE
          severity="ERROR"
          AND json_payload IS NOT NULL
        ORDER BY
          1 DESC
        LIMIT
          50

        // To find the min, max, and average latency:
        SELECT
        hour,
        MIN(took_ms) AS min,
        MAX(took_ms) AS max,
        AVG(took_ms) AS avg
        FROM (
        SELECT
          FORMAT_TIMESTAMP("%H", timestamp) AS hour,
          CAST( JSON_VALUE(json_payload,
              '$."http.resp.took_ms"') AS INT64 ) AS took_ms
        FROM
          `qwiklabs-gcp-01-cb39db4e6168.global.day2ops-log._AllLogs`
        WHERE
          timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
          AND json_payload IS NOT NULL
          AND SEARCH(labels,
            "frontend")
          AND JSON_VALUE(json_payload.message) = "request complete"
        ORDER BY
          took_ms DESC,
          timestamp ASC )
        GROUP BY
        1
        ORDER BY
        1
    
    // Product page visit number
    SELECT
    count(*)
    FROM
    `qwiklabs-gcp-01-cb39db4e6168.global.day2ops-log._AllLogs`
    WHERE
    text_payload like "GET %/product/L9ECAV7KIM %"
    AND
    timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)

    // Sessions with shopping cart checkout
    SELECT
      JSON_VALUE(json_payload.session),
      COUNT(*)
    FROM
      `qwiklabs-gcp-01-cb39db4e6168.global.day2ops-log._AllLogs`
    WHERE
      JSON_VALUE(json_payload['http.req.method']) = "POST"
      AND JSON_VALUE(json_payload['http.req.path']) = "/cart/checkout"
      AND timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
    GROUP BY
      JSON_VALUE(json_payload.session)

  Quiz
    1. You want to compare resource utilization for VMs used for production, development, and testing. What should you do?
      - Add a label called “state” to your VMs with the values “dev”, “test”, and “prod” and group by that label in your monitoring chart.

    2. Your manager wants a daily report of resource utilization by application. Where would the best export sink be?
      - GCS

      - pub/sub
      - bq

Module 5:  Working with Audit Logs
  -  In this module, we will examine how to use Cloud Audit Logs. 
  You will learn how to use Cloud Audit Logs to answer the question,
  “Who, did what, and when?” We will also cover best practices for Audit Logging.

  Learning Objectives
    - Explain Cloud Audit Logs.
    - List and explain different audit logs.
    - Explain the features and functionalities of the different audit logs.
    - List the best practices to implement audit logs.

  Cloud Audit Logs

  Data Access audit logs

  Audit logs entry format

  Best practices

  Qwiklab: Cloud Audit Logs
    - In this lab, you investigate Google Cloud Audit Logs. 
    - Cloud Audit Logging maintains multiple audit logs for each project, folder, and organization,
    - all of which help answer the question, "Who did what, when, and where?"

    Objectives
      In this lab, you learn how to:

      - Enable data access logs on Cloud Storage.
      - Generate admin and data access activity.
      - View Audit logs.

    Task 1: Enable data access logs on Cloud storage

    Task 2: Generate some admin and data access activity

      //create a Cloud Storage bucket with the same name as your project:
        gsutil mb gs://$DEVSHELL_PROJECT_ID

      // verify
        gsutil ls

      // create sample txt file contains "hello world"
        echo "Hello World!" > sample.txt
        gsutil cp sample.txt gs://$DEVSHELL_PROJECT_ID

      //verify file is in bcket
        gsutil ls gs://$DEVSHELL_PROJECT_ID

      //Create a new auto mode network named mynetwor
        gcloud compute networks create mynetwork --subnet-mode=auto
        
      // create new instance
        gcloud compute instances create default-us-vm \
--zone=europe-west4-b --network=mynetwork \
--machine-type=e2-medium

      // delete storage bucket
        gsutil rm -r gs://$DEVSHELL_PROJECT_ID
 
    Task 3: Viewing audit logs
      Expand the delete entry, then drill into protoPayload > authenticationInfo field and notice you can see the email address of the user that performed this action.

      // use Log Name to view the data_access logs
        I can say logging.google.apis.com and all data access log

  Quiz
    1. Why are the Data Access audit logs off by default? Select three.
      - They can be expensive to store
      - They can be large
      - May contain sensitive information 

    2. If you want to provide an external auditor access to your logs, what IAM role would be best?
      - Logging viewer
      
      - Project Viewer

